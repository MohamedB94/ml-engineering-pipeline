{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fca7cdba",
   "metadata": {},
   "source": [
    "# Pipeline de Data Engineering et Machine Learning\n",
    "\n",
    "Ce notebook démontre un pipeline complet de data engineering et machine learning, depuis la collecte de données jusqu'au déploiement d'une API pour servir les prédictions.\n",
    "\n",
    "Le pipeline comprend les étapes suivantes :\n",
    "1. Collecte de données (IMDb, Twitter, CSV)\n",
    "2. Nettoyage des données\n",
    "3. Feature Engineering\n",
    "4. Entraînement d'un modèle avec scikit-learn\n",
    "5. Stockage du modèle avec joblib\n",
    "6. Déploiement d'une API avec FastAPI\n",
    "\n",
    "Suivez ce notebook pour comprendre et exécuter chaque étape du processus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbecb1",
   "metadata": {},
   "source": [
    "## Installation des dépendances\n",
    "\n",
    "Commençons par installer les bibliothèques nécessaires pour notre pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f020fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installer les bibliothèques nécessaires\n",
    "# Décommentez les lignes suivantes si vous avez besoin d'installer les packages\n",
    "# !pip install numpy pandas scikit-learn matplotlib seaborn requests beautifulsoup4 joblib fastapi uvicorn tweepy nltk\n",
    "\n",
    "# Importation des bibliothèques principales\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration des visualisations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Définir le répertoire du projet directement à la racine\n",
    "project_dir = os.path.abspath(os.path.join('c:/xampp/htdocs/projet a faire pour mdmaine'))\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "print(f\"Répertoire du projet : {project_dir}\")\n",
    "print(f\"Python version : {sys.version}\")\n",
    "print(f\"Pandas version : {pd.__version__}\")\n",
    "print(f\"NumPy version : {np.__version__}\")\n",
    "\n",
    "# Créer les répertoires de données s'ils n'existent pas déjà\n",
    "os.makedirs(os.path.join(project_dir, 'data', 'raw'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'data', 'processed'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'data', 'processed', 'features'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'src'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'api'), exist_ok=True)\n",
    "os.makedirs(os.path.join(project_dir, 'notebooks'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a073454",
   "metadata": {},
   "source": [
    "## 1. Collecte de données\n",
    "\n",
    "Dans cette section, nous allons collecter des données à partir de différentes sources :\n",
    "- IMDb (données de films)\n",
    "- Twitter (données de tweets)\n",
    "- Fichiers CSV locaux\n",
    "\n",
    "Nous utiliserons les modules que nous avons développés dans le dossier `src` du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le module de collecte de données\n",
    "from src.data_collection import collect_imdb_data, collect_twitter_data, load_csv_data\n",
    "\n",
    "# Définir le chemin des données brutes\n",
    "RAW_DATA_DIR = os.path.join(project_dir, 'data', 'raw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a8e5aa",
   "metadata": {},
   "source": [
    "### 1.1 Collecte de données IMDb\n",
    "\n",
    "Récupérons des données sur quelques films populaires depuis IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e752bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste d'identifiants IMDb de films populaires\n",
    "movie_ids = [\n",
    "    'tt0111161',  # Les Évadés (The Shawshank Redemption)\n",
    "    'tt0068646',  # Le Parrain (The Godfather)\n",
    "    'tt0071562',  # Le Parrain, 2e partie (The Godfather: Part II)\n",
    "    'tt0468569',  # The Dark Knight\n",
    "    'tt0050083',  # 12 Hommes en colère (12 Angry Men)\n",
    "    'tt0108052',  # La Liste de Schindler (Schindler's List)\n",
    "    'tt0167260',  # Le Seigneur des anneaux : Le Retour du roi (LOTR: Return of the King)\n",
    "    'tt0110912',  # Pulp Fiction\n",
    "    'tt0060196',  # Le Bon, la Brute et le Truand (The Good, the Bad and the Ugly)\n",
    "    'tt0137523'   # Fight Club\n",
    "]\n",
    "\n",
    "# Collecter les données\n",
    "imdb_data = collect_imdb_data(movie_ids)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame\n",
    "if not imdb_data.empty:\n",
    "    display(imdb_data.head())\n",
    "    print(f\"Nombre de films collectés : {imdb_data.shape[0]}\")\n",
    "    print(f\"Nombre de colonnes : {imdb_data.shape[1]}\")\n",
    "else:\n",
    "    print(\"Aucune donnée n'a été collectée. Vérifiez votre clé API ou votre connexion internet.\")\n",
    "    \n",
    "    # Créer un exemple de données de films pour la démonstration si la collecte échoue\n",
    "    imdb_data = pd.DataFrame({\n",
    "        'Title': ['The Shawshank Redemption', 'The Godfather', 'The Dark Knight', 'Pulp Fiction', 'Fight Club'],\n",
    "        'Year': ['1994', '1972', '2008', '1994', '1999'],\n",
    "        'Rated': ['R', 'R', 'PG-13', 'R', 'R'],\n",
    "        'Released': ['14 Oct 1994', '24 Mar 1972', '18 Jul 2008', '14 Oct 1994', '15 Oct 1999'],\n",
    "        'Runtime': ['142 min', '175 min', '152 min', '154 min', '139 min'],\n",
    "        'Genre': ['Drama', 'Crime, Drama', 'Action, Crime, Drama', 'Crime, Drama', 'Drama'],\n",
    "        'Director': ['Frank Darabont', 'Francis Ford Coppola', 'Christopher Nolan', 'Quentin Tarantino', 'David Fincher'],\n",
    "        'Writer': ['Stephen King, Frank Darabont', 'Mario Puzo, Francis Ford Coppola', 'Jonathan Nolan, Christopher Nolan', 'Quentin Tarantino', 'Chuck Palahniuk, Jim Uhls'],\n",
    "        'Actors': ['Tim Robbins, Morgan Freeman', 'Marlon Brando, Al Pacino', 'Christian Bale, Heath Ledger', 'John Travolta, Uma Thurman', 'Brad Pitt, Edward Norton'],\n",
    "        'Plot': ['Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.', 'The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.', 'When the menace known as the Joker wreaks havoc and chaos on the people of Gotham, Batman must accept one of the greatest psychological and physical tests of his ability to fight injustice.', 'The lives of two mob hitmen, a boxer, a gangster and his wife, and a pair of diner bandits intertwine in four tales of violence and redemption.', 'An insomniac office worker and a devil-may-care soapmaker form an underground fight club that evolves into something much, much more.'],\n",
    "        'Language': ['English', 'English, Italian, Latin', 'English', 'English, Spanish, French', 'English'],\n",
    "        'Country': ['USA', 'USA', 'USA, UK', 'USA', 'USA, Germany'],\n",
    "        'Awards': ['Nominated for 7 Oscars. Another 21 wins & 32 nominations.', 'Won 3 Oscars. Another 24 wins & 28 nominations.', 'Won 2 Oscars. Another 153 wins & 159 nominations.', 'Won 1 Oscar. Another 70 wins & 75 nominations.', 'Nominated for 1 Oscar. Another 11 wins & 37 nominations.'],\n",
    "        'Poster': ['https://m.media-amazon.com/images/M/MV5BMDFkYTc0MGEtZmNhMC00ZDIzLWFmNTEtODM1ZmRlYWMwMWFmXkEyXkFqcGdeQXVyMTMxODk2OTU@._V1_SX300.jpg', 'https://m.media-amazon.com/images/M/MV5BM2MyNjYxNmUtYTAwNi00MTYxLWJmNWYtYzZlODY3ZTk3OTFlXkEyXkFqcGdeQXVyNzkwMjQ5NzM@._V1_SX300.jpg', 'https://m.media-amazon.com/images/M/MV5BMTMxNTMwODM0NF5BMl5BanBnXkFtZTcwODAyMTk2Mw@@._V1_SX300.jpg', 'https://m.media-amazon.com/images/M/MV5BNGNhMDIzZTUtNTBlZi00MTRlLWFjM2ItYzViMjE3YzI5MjljXkEyXkFqcGdeQXVyNzkwMjQ5NzM@._V1_SX300.jpg', 'https://m.media-amazon.com/images/M/MV5BMmEzNTkxYjQtZTc0MC00YTVjLTg5ZTEtZWMwOWVlYzY0NWIwXkEyXkFqcGdeQXVyNzkwMjQ5NzM@._V1_SX300.jpg'],\n",
    "        'Ratings': [{'Source': 'Internet Movie Database', 'Value': '9.3/10'}, {'Source': 'Internet Movie Database', 'Value': '9.2/10'}, {'Source': 'Internet Movie Database', 'Value': '9.0/10'}, {'Source': 'Internet Movie Database', 'Value': '8.9/10'}, {'Source': 'Internet Movie Database', 'Value': '8.8/10'}],\n",
    "        'Metascore': ['80', '100', '84', '94', '66'],\n",
    "        'imdbRating': ['9.3', '9.2', '9.0', '8.9', '8.8'],\n",
    "        'imdbVotes': ['2,400,000', '1,700,000', '2,350,000', '1,900,000', '1,850,000'],\n",
    "        'imdbID': ['tt0111161', 'tt0068646', 'tt0468569', 'tt0110912', 'tt0137523'],\n",
    "        'Type': ['movie', 'movie', 'movie', 'movie', 'movie'],\n",
    "        'DVD': ['21 Dec 1999', '11 Oct 2001', '09 Dec 2008', '19 May 1998', '14 Jun 2000'],\n",
    "        'BoxOffice': ['$28,767,189', '$135,000,000', '$534,858,444', '$107,928,762', '$37,030,102'],\n",
    "        'Production': ['Columbia Pictures, Castle Rock Entertainment', 'Paramount Pictures', 'Warner Bros., Legendary Entertainment', 'Miramax Films', '20th Century Fox, Regency Enterprises'],\n",
    "        'Website': ['N/A', 'N/A', 'N/A', 'N/A', 'N/A']\n",
    "    })\n",
    "    \n",
    "    # Sauvegarde du jeu de données d'exemple\n",
    "    imdb_data.to_csv(os.path.join(RAW_DATA_DIR, 'imdb_data.csv'), index=False)\n",
    "    display(imdb_data.head())\n",
    "    print(\"Un jeu de données d'exemple a été créé pour la démonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec501e15",
   "metadata": {},
   "source": [
    "### 1.2 Collecte de données Twitter\n",
    "\n",
    "Collectons maintenant des tweets sur un sujet spécifique (par exemple \"data science\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a868266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecter des tweets sur le thème \"data science\"\n",
    "query = \"data science\"\n",
    "\n",
    "try:\n",
    "    # Cette fonction nécessite des clés d'API Twitter configurées\n",
    "    twitter_data = collect_twitter_data(query, count=100)\n",
    "    \n",
    "    # Afficher les premières lignes si des données ont été collectées\n",
    "    if not twitter_data.empty:\n",
    "        display(twitter_data.head())\n",
    "        print(f\"Nombre de tweets collectés : {twitter_data.shape[0]}\")\n",
    "    else:\n",
    "        raise ValueError(\"Aucun tweet collecté\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Erreur lors de la collecte des tweets : {str(e)}\")\n",
    "    print(\"Création d'un jeu de données d'exemple pour la démonstration...\")\n",
    "    \n",
    "    # Créer un exemple de données de tweets pour la démonstration\n",
    "    twitter_data = pd.DataFrame({\n",
    "        'id': ['1373256789', '1373256790', '1373256791', '1373256792', '1373256793'],\n",
    "        'created_at': pd.date_range(start='2023-01-01', periods=5),\n",
    "        'text': [\n",
    "            \"I love working with #DataScience projects! The insights you can gain from data are amazing. #AI #ML\",\n",
    "            \"Just finished my latest machine learning model with scikit-learn. 95% accuracy! #DataScience #Python\",\n",
    "            \"Data Science is transforming every industry. Companies need to adapt or get left behind. #DataScience #DigitalTransformation\",\n",
    "            \"Attending a great webinar on data visualization techniques. So many ways to tell stories with data! #DataScience #DataViz\",\n",
    "            \"Struggling with this neural network architecture. Anyone have tips for image classification? #DeepLearning #DataScience\"\n",
    "        ],\n",
    "        'user': ['data_enthusiast', 'ml_expert', 'tech_journalist', 'data_viz_pro', 'ai_student'],\n",
    "        'retweets': [42, 78, 25, 18, 5],\n",
    "        'favorites': [156, 234, 87, 56, 12],\n",
    "        'hashtags': [\n",
    "            ['DataScience', 'AI', 'ML'],\n",
    "            ['DataScience', 'Python'],\n",
    "            ['DataScience', 'DigitalTransformation'],\n",
    "            ['DataScience', 'DataViz'],\n",
    "            ['DeepLearning', 'DataScience']\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Sauvegarde du jeu de données d'exemple\n",
    "    twitter_data.to_csv(os.path.join(RAW_DATA_DIR, 'twitter_data_science.csv'), index=False)\n",
    "    display(twitter_data.head())\n",
    "    print(\"Un jeu de données d'exemple a été créé pour la démonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab8a7e0",
   "metadata": {},
   "source": [
    "### 1.3 Chargement de données CSV\n",
    "\n",
    "Démontrons comment charger des données à partir d'un fichier CSV :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un petit dataset d'exemple si besoin\n",
    "sample_data_path = os.path.join(RAW_DATA_DIR, 'sample_data.csv')\n",
    "\n",
    "if not os.path.exists(sample_data_path):\n",
    "    # Créer un jeu de données d'exemple\n",
    "    sample_data = pd.DataFrame({\n",
    "        'user_id': range(1, 101),\n",
    "        'age': np.random.randint(18, 65, 100),\n",
    "        'gender': np.random.choice(['M', 'F', 'Other'], 100),\n",
    "        'income': np.random.normal(50000, 15000, 100),\n",
    "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 100),\n",
    "        'movie_genre_preference': np.random.choice(['Action', 'Comedy', 'Drama', 'Sci-Fi', 'Horror'], 100),\n",
    "        'rating_frequency': np.random.randint(1, 50, 100),\n",
    "        'last_active': pd.date_range(start='2023-01-01', periods=100)\n",
    "    })\n",
    "    \n",
    "    # Ajouter quelques valeurs manquantes\n",
    "    sample_data.loc[np.random.choice(sample_data.index, 10), 'income'] = np.nan\n",
    "    sample_data.loc[np.random.choice(sample_data.index, 5), 'age'] = np.nan\n",
    "    \n",
    "    # Sauvegarder les données\n",
    "    sample_data.to_csv(sample_data_path, index=False)\n",
    "    print(f\"Jeu de données d'exemple créé et sauvegardé dans {sample_data_path}\")\n",
    "else:\n",
    "    print(f\"Le fichier {sample_data_path} existe déjà\")\n",
    "\n",
    "# Charger les données avec notre fonction\n",
    "csv_data = load_csv_data(sample_data_path)\n",
    "\n",
    "# Afficher les premières lignes\n",
    "display(csv_data.head())\n",
    "print(f\"Nombre d'enregistrements : {csv_data.shape[0]}\")\n",
    "print(f\"Nombre de colonnes : {csv_data.shape[1]}\")\n",
    "\n",
    "# Afficher des informations sur les données\n",
    "print(\"\\nInformations sur les données :\")\n",
    "print(csv_data.info())\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\nStatistiques descriptives :\")\n",
    "display(csv_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182d0ec",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données\n",
    "\n",
    "Dans cette section, nous allons nettoyer les données collectées en :\n",
    "- Gérant les valeurs manquantes\n",
    "- Supprimant les doublons\n",
    "- Normalisant les formats\n",
    "- Convertissant les types de données\n",
    "\n",
    "Nous utiliserons les modules que nous avons développés dans le dossier `src` du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le module de nettoyage des données\n",
    "from src.data_cleaning import clean_imdb_data, clean_twitter_data, clean_csv_data\n",
    "\n",
    "# Définir les chemins des répertoires\n",
    "PROCESSED_DATA_DIR = os.path.join(project_dir, 'data', 'processed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5217de2",
   "metadata": {},
   "source": [
    "### 2.1 Nettoyage des données IMDb\n",
    "\n",
    "Nettoyons les données IMDb collectées précédemment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0029dbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyer les données IMDb\n",
    "clean_imdb = clean_imdb_data('imdb_data.csv')\n",
    "\n",
    "# Afficher les données nettoyées\n",
    "if clean_imdb is not None:\n",
    "    display(clean_imdb.head())\n",
    "    \n",
    "    # Visualiser les valeurs manquantes\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(clean_imdb.isnull(), cbar=False, yticklabels=False, cmap='viridis')\n",
    "    plt.title('Valeurs manquantes dans les données IMDb nettoyées')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Comparaison avant/après pour certaines colonnes\n",
    "    if 'imdbRating' in clean_imdb.columns and 'imdbRating' in imdb_data.columns:\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Avant nettoyage\n",
    "        ax[0].hist(pd.to_numeric(imdb_data['imdbRating'], errors='coerce'), bins=10, alpha=0.7)\n",
    "        ax[0].set_title('Notes IMDb avant nettoyage')\n",
    "        ax[0].set_xlabel('Note')\n",
    "        ax[0].set_ylabel('Fréquence')\n",
    "        \n",
    "        # Après nettoyage\n",
    "        ax[1].hist(clean_imdb['imdbRating'], bins=10, alpha=0.7, color='green')\n",
    "        ax[1].set_title('Notes IMDb après nettoyage')\n",
    "        ax[1].set_xlabel('Note')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Échec du nettoyage des données IMDb.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f9f86a",
   "metadata": {},
   "source": [
    "### 2.2 Nettoyage des données Twitter\n",
    "\n",
    "Nettoyons maintenant les données Twitter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99df709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier le fichier Twitter à nettoyer\n",
    "twitter_files = [f for f in os.listdir(RAW_DATA_DIR) if f.startswith('twitter_')]\n",
    "\n",
    "if twitter_files:\n",
    "    # Nettoyer le premier fichier trouvé\n",
    "    twitter_file = twitter_files[0]\n",
    "    clean_twitter = clean_twitter_data(twitter_file)\n",
    "    \n",
    "    if clean_twitter is not None:\n",
    "        display(clean_twitter.head())\n",
    "        \n",
    "        # Visualiser les longueurs de tweets avant et après nettoyage\n",
    "        if 'text' in twitter_data.columns and 'clean_text' in clean_twitter.columns:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "            \n",
    "            # Avant nettoyage\n",
    "            tweet_lengths = twitter_data['text'].str.len()\n",
    "            ax[0].hist(tweet_lengths, bins=20, alpha=0.7)\n",
    "            ax[0].set_title('Longueur des tweets avant nettoyage')\n",
    "            ax[0].set_xlabel('Nombre de caractères')\n",
    "            ax[0].set_ylabel('Fréquence')\n",
    "            \n",
    "            # Après nettoyage\n",
    "            clean_tweet_lengths = clean_twitter['clean_text'].str.len()\n",
    "            ax[1].hist(clean_tweet_lengths, bins=20, alpha=0.7, color='green')\n",
    "            ax[1].set_title('Longueur des tweets après nettoyage')\n",
    "            ax[1].set_xlabel('Nombre de caractères')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Comparer un exemple de tweet avant et après nettoyage\n",
    "            print(\"Exemple de tweet avant et après nettoyage :\")\n",
    "            sample_idx = 0\n",
    "            original_tweet = twitter_data.iloc[sample_idx]['text']\n",
    "            cleaned_tweet = clean_twitter.iloc[sample_idx]['clean_text']\n",
    "            \n",
    "            print(f\"Original: {original_tweet}\")\n",
    "            print(f\"Nettoyé : {cleaned_tweet}\")\n",
    "else:\n",
    "    print(\"Aucun fichier Twitter trouvé pour le nettoyage.\")\n",
    "    print(\"Création d'un exemple de données Twitter nettoyées...\")\n",
    "    \n",
    "    # Créer un exemple de données Twitter nettoyées\n",
    "    clean_twitter = pd.DataFrame({\n",
    "        'id': ['1373256789', '1373256790', '1373256791', '1373256792', '1373256793'],\n",
    "        'created_at': pd.date_range(start='2023-01-01', periods=5),\n",
    "        'text': [\n",
    "            \"I love working with #DataScience projects! The insights you can gain from data are amazing. #AI #ML\",\n",
    "            \"Just finished my latest machine learning model with scikit-learn. 95% accuracy! #DataScience #Python\",\n",
    "            \"Data Science is transforming every industry. Companies need to adapt or get left behind. #DataScience #DigitalTransformation\",\n",
    "            \"Attending a great webinar on data visualization techniques. So many ways to tell stories with data! #DataScience #DataViz\",\n",
    "            \"Struggling with this neural network architecture. Anyone have tips for image classification? #DeepLearning #DataScience\"\n",
    "        ],\n",
    "        'clean_text': [\n",
    "            \"I love working with DataScience projects The insights you can gain from data are amazing\",\n",
    "            \"Just finished my latest machine learning model with scikitlearn 95 accuracy\",\n",
    "            \"Data Science is transforming every industry Companies need to adapt or get left behind\",\n",
    "            \"Attending a great webinar on data visualization techniques So many ways to tell stories with data\",\n",
    "            \"Struggling with this neural network architecture Anyone have tips for image classification\"\n",
    "        ],\n",
    "        'user': ['data_enthusiast', 'ml_expert', 'tech_journalist', 'data_viz_pro', 'ai_student'],\n",
    "        'retweets': [42, 78, 25, 18, 5],\n",
    "        'favorites': [156, 234, 87, 56, 12],\n",
    "        'tweet_length': [107, 92, 105, 112, 115]\n",
    "    })\n",
    "    \n",
    "    # Sauvegarde dans le dossier processed\n",
    "    clean_twitter.to_csv(os.path.join(PROCESSED_DATA_DIR, 'clean_twitter_data_science.csv'), index=False)\n",
    "    display(clean_twitter.head())\n",
    "    print(\"Un exemple de données Twitter nettoyées a été créé pour la démonstration.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd220c",
   "metadata": {},
   "source": [
    "### 2.3 Nettoyage des données CSV\n",
    "\n",
    "Nettoyons maintenant les données CSV avec des transformations spécifiques :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851bde95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le fichier sample_data.csv existe\n",
    "if os.path.exists(os.path.join(RAW_DATA_DIR, 'sample_data.csv')):\n",
    "    # Définir les configurations de nettoyage\n",
    "    date_columns = ['last_active']\n",
    "    numeric_columns = ['age', 'income', 'rating_frequency']\n",
    "    categorical_columns = ['gender', 'education', 'movie_genre_preference']\n",
    "    \n",
    "    # Nettoyer les données\n",
    "    clean_sample_data = clean_csv_data(\n",
    "        'sample_data.csv', \n",
    "        date_columns=date_columns,\n",
    "        numeric_columns=numeric_columns,\n",
    "        categorical_columns=categorical_columns\n",
    "    )\n",
    "    \n",
    "    if clean_sample_data is not None:\n",
    "        display(clean_sample_data.head())\n",
    "        \n",
    "        # Vérifier les types de données après nettoyage\n",
    "        print(\"Types de données après nettoyage :\")\n",
    "        print(clean_sample_data.dtypes)\n",
    "        \n",
    "        # Visualiser l'effet du nettoyage sur les valeurs manquantes\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        \n",
    "        # Avant nettoyage\n",
    "        sns.heatmap(csv_data.isnull(), cbar=False, yticklabels=False, cmap='viridis', ax=ax[0])\n",
    "        ax[0].set_title('Valeurs manquantes avant nettoyage')\n",
    "        \n",
    "        # Après nettoyage\n",
    "        sns.heatmap(clean_sample_data.isnull(), cbar=False, yticklabels=False, cmap='viridis', ax=ax[1])\n",
    "        ax[1].set_title('Valeurs manquantes après nettoyage')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Visualiser la distribution d'une variable numérique\n",
    "        if 'income' in clean_sample_data.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(clean_sample_data['income'], kde=True)\n",
    "            plt.title('Distribution des revenus après nettoyage')\n",
    "            plt.xlabel('Revenu')\n",
    "            plt.ylabel('Fréquence')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Fichier sample_data.csv non trouvé.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7630f1bd",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Dans cette section, nous allons créer de nouvelles caractéristiques à partir des données nettoyées pour améliorer les performances des modèles. Les transformations incluent :\n",
    "- Encodage des variables catégorielles\n",
    "- Extraction de caractéristiques à partir du texte\n",
    "- Normalisation et standardisation des données\n",
    "- Création de caractéristiques dérivées\n",
    "\n",
    "Nous utiliserons les modules que nous avons développés dans le dossier `src` du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2715f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le module de feature engineering\n",
    "from src.feature_engineering import engineer_imdb_features, engineer_twitter_features, engineer_custom_features\n",
    "\n",
    "# Définir le chemin du répertoire des caractéristiques\n",
    "FEATURES_DATA_DIR = os.path.join(PROCESSED_DATA_DIR, 'features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58793552",
   "metadata": {},
   "source": [
    "### 3.1 Feature Engineering pour les données IMDb\n",
    "\n",
    "Créons de nouvelles caractéristiques pour les données IMDb :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b587e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le fichier de données nettoyées existe\n",
    "if os.path.exists(os.path.join(PROCESSED_DATA_DIR, 'clean_imdb_data.csv')):\n",
    "    # Créer de nouvelles caractéristiques\n",
    "    featured_imdb = engineer_imdb_features()\n",
    "    \n",
    "    if featured_imdb is not None:\n",
    "        # Afficher les données avec les nouvelles caractéristiques\n",
    "        print(\"Caractéristiques des données IMDb après feature engineering :\")\n",
    "        print(f\"Nombre de lignes : {featured_imdb.shape[0]}\")\n",
    "        print(f\"Nombre de colonnes : {featured_imdb.shape[1]}\")\n",
    "        print(\"\\nNouvelles colonnes :\")\n",
    "        \n",
    "        # Identifier les nouvelles colonnes ajoutées\n",
    "        clean_columns = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, 'clean_imdb_data.csv')).columns.tolist()\n",
    "        new_columns = [col for col in featured_imdb.columns if col not in clean_columns]\n",
    "        \n",
    "        print(new_columns)\n",
    "        \n",
    "        # Afficher les premières lignes\n",
    "        display(featured_imdb[new_columns].head())\n",
    "        \n",
    "        # Visualiser quelques nouvelles caractéristiques\n",
    "        if 'movie_age' in featured_imdb.columns and 'imdbRating' in featured_imdb.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(featured_imdb['movie_age'], featured_imdb['imdbRating'], alpha=0.7)\n",
    "            plt.title('Relation entre l\\'âge du film et sa note IMDb')\n",
    "            plt.xlabel('Âge du film (années)')\n",
    "            plt.ylabel('Note IMDb')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualiser la distribution d'une nouvelle caractéristique\n",
    "        if 'plot_sentiment' in featured_imdb.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(featured_imdb['plot_sentiment'], kde=True)\n",
    "            plt.title('Distribution du sentiment des résumés de films')\n",
    "            plt.xlabel('Score de sentiment (-1 = négatif, 1 = positif)')\n",
    "            plt.ylabel('Fréquence')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Relation entre le sentiment du résumé et la note\n",
    "            if 'imdbRating' in featured_imdb.columns:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(featured_imdb['plot_sentiment'], featured_imdb['imdbRating'], alpha=0.7)\n",
    "                plt.title('Relation entre le sentiment du résumé et la note IMDb')\n",
    "                plt.xlabel('Score de sentiment du résumé')\n",
    "                plt.ylabel('Note IMDb')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "else:\n",
    "    print(\"Le fichier clean_imdb_data.csv n'existe pas. Impossible de procéder au feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d804e0",
   "metadata": {},
   "source": [
    "### 3.2 Feature Engineering pour les données Twitter\n",
    "\n",
    "Créons de nouvelles caractéristiques pour les données Twitter :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c649b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechercher les fichiers Twitter nettoyés\n",
    "clean_twitter_files = [f for f in os.listdir(PROCESSED_DATA_DIR) if f.startswith('clean_twitter_')]\n",
    "\n",
    "if clean_twitter_files:\n",
    "    # Prendre le premier fichier trouvé\n",
    "    clean_twitter_file = clean_twitter_files[0]\n",
    "    \n",
    "    # Générer les caractéristiques\n",
    "    featured_twitter = engineer_twitter_features(clean_twitter_file)\n",
    "    \n",
    "    if featured_twitter is not None:\n",
    "        # Afficher les données avec les nouvelles caractéristiques\n",
    "        print(\"Caractéristiques des données Twitter après feature engineering :\")\n",
    "        print(f\"Nombre de lignes : {featured_twitter.shape[0]}\")\n",
    "        print(f\"Nombre de colonnes : {featured_twitter.shape[1]}\")\n",
    "        print(\"\\nNouvelles colonnes :\")\n",
    "        \n",
    "        # Identifier les nouvelles colonnes ajoutées\n",
    "        clean_columns = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, clean_twitter_file)).columns.tolist()\n",
    "        new_columns = [col for col in featured_twitter.columns if col not in clean_columns]\n",
    "        \n",
    "        print(new_columns)\n",
    "        \n",
    "        # Afficher les premières lignes\n",
    "        display(featured_twitter[new_columns].head())\n",
    "        \n",
    "        # Visualiser les scores de sentiment\n",
    "        if 'sentiment_score' in featured_twitter.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.histplot(featured_twitter['sentiment_score'], kde=True, bins=20)\n",
    "            plt.title('Distribution des scores de sentiment des tweets')\n",
    "            plt.xlabel('Score de sentiment (-1 = négatif, 1 = positif)')\n",
    "            plt.ylabel('Fréquence')\n",
    "            plt.axvline(x=0, color='red', linestyle='--')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Visualiser la relation entre le sentiment et l'engagement\n",
    "            if 'engagement' in featured_twitter.columns:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.scatter(featured_twitter['sentiment_score'], featured_twitter['engagement'], alpha=0.7)\n",
    "                plt.title('Relation entre le sentiment et l\\'engagement')\n",
    "                plt.xlabel('Score de sentiment')\n",
    "                plt.ylabel('Engagement (retweets + favoris)')\n",
    "                plt.grid(True, linestyle='--', alpha=0.7)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        # Visualiser la répartition des tweets par heure de la journée\n",
    "        if 'hour_of_day' in featured_twitter.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.countplot(data=featured_twitter, x='hour_of_day')\n",
    "            plt.title('Répartition des tweets par heure de la journée')\n",
    "            plt.xlabel('Heure de la journée')\n",
    "            plt.ylabel('Nombre de tweets')\n",
    "            plt.xticks(range(0, 24))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(\"Aucun fichier Twitter nettoyé trouvé. Création de données d'exemple...\")\n",
    "    \n",
    "    # Créer un exemple de données Twitter avec caractéristiques\n",
    "    featured_twitter = pd.DataFrame({\n",
    "        'id': ['1373256789', '1373256790', '1373256791', '1373256792', '1373256793'],\n",
    "        'created_at': pd.date_range(start='2023-01-01', periods=5),\n",
    "        'text': [\n",
    "            \"I love working with #DataScience projects! The insights you can gain from data are amazing. #AI #ML\",\n",
    "            \"Just finished my latest machine learning model with scikit-learn. 95% accuracy! #DataScience #Python\",\n",
    "            \"Data Science is transforming every industry. Companies need to adapt or get left behind. #DataScience #DigitalTransformation\",\n",
    "            \"Attending a great webinar on data visualization techniques. So many ways to tell stories with data! #DataScience #DataViz\",\n",
    "            \"Struggling with this neural network architecture. Anyone have tips for image classification? #DeepLearning #DataScience\"\n",
    "        ],\n",
    "        'clean_text': [\n",
    "            \"I love working with DataScience projects The insights you can gain from data are amazing\",\n",
    "            \"Just finished my latest machine learning model with scikitlearn 95 accuracy\",\n",
    "            \"Data Science is transforming every industry Companies need to adapt or get left behind\",\n",
    "            \"Attending a great webinar on data visualization techniques So many ways to tell stories with data\",\n",
    "            \"Struggling with this neural network architecture Anyone have tips for image classification\"\n",
    "        ],\n",
    "        'user': ['data_enthusiast', 'ml_expert', 'tech_journalist', 'data_viz_pro', 'ai_student'],\n",
    "        'retweets': [42, 78, 25, 18, 5],\n",
    "        'favorites': [156, 234, 87, 56, 12],\n",
    "        'hour_of_day': [9, 14, 11, 16, 22],\n",
    "        'day_of_week': [0, 2, 4, 1, 6],\n",
    "        'is_weekend': [0, 0, 0, 0, 1],\n",
    "        'sentiment_score': [0.78, 0.65, 0.42, 0.56, -0.23],\n",
    "        'sentiment_positive': [1, 1, 1, 1, 0],\n",
    "        'sentiment_negative': [0, 0, 0, 0, 1],\n",
    "        'sentiment_neutral': [0, 0, 0, 0, 0],\n",
    "        'word_count': [15, 11, 13, 16, 12],\n",
    "        'has_question': [0, 0, 0, 0, 1],\n",
    "        'has_exclamation': [1, 1, 0, 1, 0],\n",
    "        'capital_letter_ratio': [0.08, 0.05, 0.12, 0.06, 0.04],\n",
    "        'engagement': [198, 312, 112, 74, 17],\n",
    "        'retweet_to_favorite_ratio': [0.27, 0.33, 0.29, 0.32, 0.42]\n",
    "    })\n",
    "    \n",
    "    # Sauvegarde dans le dossier des caractéristiques\n",
    "    os.makedirs(FEATURES_DATA_DIR, exist_ok=True)\n",
    "    featured_twitter.to_csv(os.path.join(FEATURES_DATA_DIR, 'featured_twitter_data_science.csv'), index=False)\n",
    "    \n",
    "    display(featured_twitter.head())\n",
    "    print(\"Un exemple de données Twitter avec caractéristiques a été créé pour la démonstration.\")\n",
    "    \n",
    "    # Visualiser quelques caractéristiques\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(featured_twitter['sentiment_score'], kde=True, bins=10)\n",
    "    plt.title('Distribution des scores de sentiment des tweets (données d\\'exemple)')\n",
    "    plt.xlabel('Score de sentiment (-1 = négatif, 1 = positif)')\n",
    "    plt.ylabel('Fréquence')\n",
    "    plt.axvline(x=0, color='red', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce1a46",
   "metadata": {},
   "source": [
    "### 3.3 Feature Engineering pour les données CSV\n",
    "\n",
    "Créons maintenant des caractéristiques personnalisées pour nos données CSV :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9e4058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le fichier de données nettoyées existe\n",
    "clean_sample_csv = 'clean_sample_data.csv'\n",
    "if os.path.exists(os.path.join(PROCESSED_DATA_DIR, clean_sample_csv)):\n",
    "    # Définir la configuration de feature engineering\n",
    "    config = {\n",
    "        'scale_columns': ['age', 'income', 'rating_frequency'],\n",
    "        'onehot_columns': ['gender', 'education', 'movie_genre_preference'],\n",
    "        'bin_columns': [\n",
    "            {'column': 'age', 'bins': 4},\n",
    "            {'column': 'income', 'bins': 5}\n",
    "        ],\n",
    "        'interactions': [\n",
    "            ('age', 'rating_frequency'),\n",
    "            ('income', 'rating_frequency')\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Générer les caractéristiques\n",
    "    featured_csv = engineer_custom_features(clean_sample_csv, config)\n",
    "    \n",
    "    if featured_csv is not None:\n",
    "        # Afficher les données avec les nouvelles caractéristiques\n",
    "        print(\"Caractéristiques des données CSV après feature engineering :\")\n",
    "        print(f\"Nombre de lignes : {featured_csv.shape[0]}\")\n",
    "        print(f\"Nombre de colonnes : {featured_csv.shape[1]}\")\n",
    "        print(\"\\nNouvelles colonnes :\")\n",
    "        \n",
    "        # Identifier les nouvelles colonnes ajoutées\n",
    "        clean_columns = pd.read_csv(os.path.join(PROCESSED_DATA_DIR, clean_sample_csv)).columns.tolist()\n",
    "        new_columns = [col for col in featured_csv.columns if col not in clean_columns]\n",
    "        \n",
    "        print(new_columns)\n",
    "        \n",
    "        # Afficher les premières lignes\n",
    "        display(featured_csv[new_columns].head())\n",
    "        \n",
    "        # Visualiser les variables standardisées\n",
    "        if 'age_scaled' in featured_csv.columns and 'income_scaled' in featured_csv.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.scatter(featured_csv['age_scaled'], featured_csv['income_scaled'], alpha=0.7)\n",
    "            plt.title('Variables standardisées: Âge vs Revenu')\n",
    "            plt.xlabel('Âge (standardisé)')\n",
    "            plt.ylabel('Revenu (standardisé)')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualiser les variables discrétisées (binned)\n",
    "        if 'age_binned' in featured_csv.columns:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.countplot(data=featured_csv, x='age_binned')\n",
    "            plt.title('Répartition des tranches d\\'âge')\n",
    "            plt.xlabel('Tranche d\\'âge')\n",
    "            plt.ylabel('Nombre d\\'utilisateurs')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Visualiser le one-hot encoding d'une variable catégorielle\n",
    "        onehot_cols = [col for col in new_columns if col.startswith('gender_')]\n",
    "        if onehot_cols:\n",
    "            onehot_sum = featured_csv[onehot_cols].sum()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            onehot_sum.plot(kind='bar')\n",
    "            plt.title('Répartition des genres après one-hot encoding')\n",
    "            plt.xlabel('Genre')\n",
    "            plt.ylabel('Nombre d\\'utilisateurs')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "else:\n",
    "    print(f\"Le fichier {clean_sample_csv} n'existe pas. Impossible de procéder au feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607d316d",
   "metadata": {},
   "source": [
    "## 4. Entraînement d'un modèle avec scikit-learn\n",
    "\n",
    "Dans cette section, nous allons entraîner un modèle de machine learning en utilisant scikit-learn. Nous allons :\n",
    "- Préparer les données pour l'entraînement (division train/test)\n",
    "- Sélectionner et entraîner un modèle\n",
    "- Évaluer les performances du modèle\n",
    "- Analyser les caractéristiques importantes\n",
    "\n",
    "Nous utiliserons les modules que nous avons développés dans le dossier `src` du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3c4db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer le module d'entraînement de modèle\n",
    "from src.model_training import load_training_data, train_regression_model, train_classification_model, save_model\n",
    "\n",
    "# Importer scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Définir le chemin des modèles\n",
    "MODELS_DIR = os.path.join(project_dir, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74d3a2",
   "metadata": {},
   "source": [
    "### 4.1 Entraînement d'un modèle de régression pour prédire les notes IMDb\n",
    "\n",
    "Entraînons un modèle de régression pour prédire les notes IMDb à partir des caractéristiques générées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d67cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier si le fichier de données avec caractéristiques existe\n",
    "imdb_feature_file = 'featured_imdb_data.csv'\n",
    "imdb_feature_path = os.path.join(FEATURES_DATA_DIR, imdb_feature_file)\n",
    "\n",
    "if os.path.exists(imdb_feature_path):\n",
    "    # Charger les données\n",
    "    X_train, X_test, y_train, y_test, features = load_training_data(\n",
    "        imdb_feature_file,\n",
    "        target_column='imdbRating',\n",
    "        features=None,  # Utiliser toutes les colonnes numériques\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    if X_train is not None:\n",
    "        print(f\"Données d'entraînement: {X_train.shape[0]} exemples, {X_train.shape[1]} caractéristiques\")\n",
    "        print(f\"Données de test: {X_test.shape[0]} exemples, {X_test.shape[1]} caractéristiques\")\n",
    "        \n",
    "        # Entraîner différents modèles de régression\n",
    "        models = ['linear', 'random_forest', 'gradient_boosting']\n",
    "        performances = {}\n",
    "        best_model = None\n",
    "        best_score = -float('inf')\n",
    "        \n",
    "        for model_type in models:\n",
    "            print(f\"\\nEntraînement du modèle {model_type}...\")\n",
    "            model, performance = train_regression_model(\n",
    "                X_train, y_train, X_test, y_test, features,\n",
    "                model_type=model_type\n",
    "            )\n",
    "            \n",
    "            performances[model_type] = performance\n",
    "            \n",
    "            # Conserver le meilleur modèle selon le R²\n",
    "            if performance['r2'] > best_score:\n",
    "                best_model = model\n",
    "                best_score = performance['r2']\n",
    "                best_model_type = model_type\n",
    "        \n",
    "        # Comparer les performances des modèles\n",
    "        print(\"\\nComparaison des performances des modèles :\")\n",
    "        metrics = ['mse', 'rmse', 'r2']\n",
    "        models_df = pd.DataFrame({\n",
    "            model_type: [performances[model_type][metric] for metric in metrics]\n",
    "            for model_type in models\n",
    "        }, index=metrics)\n",
    "        \n",
    "        display(models_df)\n",
    "        \n",
    "        # Visualiser les performances\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        models_df.loc['r2'].plot(kind='bar')\n",
    "        plt.title('Comparaison des scores R² des modèles')\n",
    "        plt.ylabel('Score R²')\n",
    "        plt.ylim(0, 1)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        if best_model is not None:\n",
    "            model_path = save_model(best_model, 'imdb_rating_predictor', {\n",
    "                'performance': performances[best_model_type],\n",
    "                'features': features,\n",
    "                'target': 'imdbRating',\n",
    "                'model_type': best_model_type\n",
    "            })\n",
    "            \n",
    "            print(f\"\\nMeilleur modèle ({best_model_type}) sauvegardé dans {model_path}\")\n",
    "else:\n",
    "    print(f\"Le fichier {imdb_feature_path} n'existe pas. Impossible d'entraîner le modèle.\")\n",
    "    \n",
    "    # Créer un jeu de données fictif pour la démonstration\n",
    "    print(\"Création d'un jeu de données d'exemple pour la démonstration...\")\n",
    "    \n",
    "    # Générer des données synthétiques\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    X = np.random.rand(n_samples, 5)\n",
    "    y = 5 + 2 * X[:, 0] + 3 * X[:, 1] - 1.5 * X[:, 2] + 0.5 * X[:, 3] + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    feature_names = ['movie_age', 'actor_count', 'runtime', 'budget', 'popularity']\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y, name='imdbRating')\n",
    "    \n",
    "    # Division en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_series, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Données synthétiques créées: {X_train.shape[0]} exemples d'entraînement, {X_test.shape[0]} exemples de test\")\n",
    "    \n",
    "    # Entraîner un modèle de forêt aléatoire\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluer le modèle\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Performance du modèle synthétique:\")\n",
    "    print(f\"  MSE: {mse:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  R²: {r2:.4f}\")\n",
    "    \n",
    "    # Visualiser les prédictions vs valeurs réelles\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Valeurs réelles')\n",
    "    plt.ylabel('Prédictions')\n",
    "    plt.title('Prédictions vs Valeurs réelles (données synthétiques)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Importance des caractéristiques\n",
    "    feature_importances = model.feature_importances_\n",
    "    features_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "    features_df = features_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(features_df['feature'], features_df['importance'])\n",
    "    plt.title('Importance des caractéristiques (données synthétiques)')\n",
    "    plt.xlabel('Caractéristique')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    model_path = os.path.join(MODELS_DIR, 'demo_imdb_rating_predictor.joblib')\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Modèle de démonstration sauvegardé dans {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb6c70",
   "metadata": {},
   "source": [
    "### 4.2 Entraînement d'un modèle de classification pour le sentiment des tweets\n",
    "\n",
    "Entraînons un modèle de classification pour prédire le sentiment des tweets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rechercher les fichiers Twitter avec caractéristiques\n",
    "twitter_feature_files = [f for f in os.listdir(FEATURES_DATA_DIR) if f.startswith('featured_') and 'twitter' in f.lower()]\n",
    "\n",
    "if twitter_feature_files:\n",
    "    # Prendre le premier fichier trouvé\n",
    "    twitter_feature_file = twitter_feature_files[0]\n",
    "    \n",
    "    # Vérifier si la cible existe\n",
    "    twitter_df = pd.read_csv(os.path.join(FEATURES_DATA_DIR, twitter_feature_file))\n",
    "    \n",
    "    if 'sentiment_positive' in twitter_df.columns:\n",
    "        # Charger les données\n",
    "        X_train, X_test, y_train, y_test, features = load_training_data(\n",
    "            twitter_feature_file,\n",
    "            target_column='sentiment_positive',\n",
    "            features=None,  # Utiliser toutes les colonnes numériques\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        if X_train is not None:\n",
    "            print(f\"Données d'entraînement: {X_train.shape[0]} exemples, {X_train.shape[1]} caractéristiques\")\n",
    "            print(f\"Données de test: {X_test.shape[0]} exemples, {X_test.shape[1]} caractéristiques\")\n",
    "            \n",
    "            # Entraîner différents modèles de classification\n",
    "            models = ['logistic', 'random_forest', 'gradient_boosting']\n",
    "            performances = {}\n",
    "            best_model = None\n",
    "            best_score = -float('inf')\n",
    "            \n",
    "            for model_type in models:\n",
    "                print(f\"\\nEntraînement du modèle {model_type}...\")\n",
    "                model, performance = train_classification_model(\n",
    "                    X_train, y_train, X_test, y_test, features,\n",
    "                    model_type=model_type\n",
    "                )\n",
    "                \n",
    "                performances[model_type] = performance\n",
    "                \n",
    "                # Conserver le meilleur modèle selon le F1-score\n",
    "                if performance['f1'] > best_score:\n",
    "                    best_model = model\n",
    "                    best_score = performance['f1']\n",
    "                    best_model_type = model_type\n",
    "            \n",
    "            # Comparer les performances des modèles\n",
    "            print(\"\\nComparaison des performances des modèles :\")\n",
    "            metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "            models_df = pd.DataFrame({\n",
    "                model_type: [performances[model_type][metric] for metric in metrics]\n",
    "                for model_type in models\n",
    "            }, index=metrics)\n",
    "            \n",
    "            display(models_df)\n",
    "            \n",
    "            # Visualiser les performances\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            models_df.plot(kind='bar')\n",
    "            plt.title('Comparaison des performances des modèles')\n",
    "            plt.ylabel('Score')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Sauvegarder le meilleur modèle\n",
    "            if best_model is not None:\n",
    "                model_path = save_model(best_model, 'twitter_sentiment_classifier', {\n",
    "                    'performance': performances[best_model_type],\n",
    "                    'features': features,\n",
    "                    'target': 'sentiment_positive',\n",
    "                    'model_type': best_model_type\n",
    "                })\n",
    "                \n",
    "                print(f\"\\nMeilleur modèle ({best_model_type}) sauvegardé dans {model_path}\")\n",
    "    else:\n",
    "        print(\"La colonne 'sentiment_positive' n'existe pas dans les données Twitter.\")\n",
    "else:\n",
    "    print(\"Aucun fichier Twitter avec caractéristiques trouvé.\")\n",
    "    \n",
    "    # Créer un jeu de données fictif pour la démonstration\n",
    "    print(\"Création d'un jeu de données d'exemple pour la démonstration...\")\n",
    "    \n",
    "    # Générer des données synthétiques\n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    \n",
    "    X = np.random.rand(n_samples, 4)\n",
    "    # Générer une cible binaire avec une relation logique\n",
    "    y = (0.8 * X[:, 0] - 0.5 * X[:, 1] + 0.3 * X[:, 2] - 0.1 * X[:, 3] > 0.5).astype(int)\n",
    "    \n",
    "    feature_names = ['sentiment_score', 'word_count', 'capital_letter_ratio', 'has_exclamation']\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y, name='sentiment_positive')\n",
    "    \n",
    "    # Division en ensembles d'entraînement et de test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_df, y_series, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(f\"Données synthétiques créées: {X_train.shape[0]} exemples d'entraînement, {X_test.shape[0]} exemples de test\")\n",
    "    \n",
    "    # Entraîner un modèle de forêt aléatoire\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Évaluer le modèle\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Performance du modèle synthétique:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Visualiser la matrice de confusion\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.xlabel('Prédictions')\n",
    "    plt.ylabel('Valeurs réelles')\n",
    "    plt.title('Matrice de confusion (données synthétiques)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Importance des caractéristiques\n",
    "    feature_importances = model.feature_importances_\n",
    "    features_df = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "    features_df = features_df.sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(features_df['feature'], features_df['importance'])\n",
    "    plt.title('Importance des caractéristiques (données synthétiques)')\n",
    "    plt.xlabel('Caractéristique')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Sauvegarder le modèle\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    model_path = os.path.join(MODELS_DIR, 'demo_twitter_sentiment_classifier.joblib')\n",
    "    \n",
    "    import joblib\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Modèle de démonstration sauvegardé dans {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f7411",
   "metadata": {},
   "source": [
    "## 5. Stockage du modèle avec joblib\n",
    "\n",
    "Dans cette section, nous allons explorer comment stocker et charger des modèles entraînés à l'aide de la bibliothèque joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5b3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer joblib\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Créer un modèle simple pour la démonstration si nécessaire\n",
    "if not os.path.exists(os.path.join(MODELS_DIR, 'demo_imdb_rating_predictor.joblib')):\n",
    "    # Créer un modèle de forêt aléatoire simple\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    \n",
    "    # Entraîner sur des données synthétiques\n",
    "    np.random.seed(42)\n",
    "    X = np.random.rand(100, 5)\n",
    "    y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + np.random.normal(0, 0.1, 100)\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Sauvegarder dans le dossier des modèles\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    joblib.dump(model, os.path.join(MODELS_DIR, 'demo_imdb_rating_predictor.joblib'))\n",
    "    print(\"Modèle de démonstration créé.\")\n",
    "\n",
    "# Lister les modèles disponibles\n",
    "model_files = [f for f in os.listdir(MODELS_DIR) if f.endswith('.joblib')]\n",
    "print(f\"Modèles disponibles: {model_files}\")\n",
    "\n",
    "# Sélectionner un modèle à charger (le premier disponible)\n",
    "if model_files:\n",
    "    selected_model = model_files[0]\n",
    "    model_path = os.path.join(MODELS_DIR, selected_model)\n",
    "    \n",
    "    print(f\"\\nChargement du modèle: {selected_model}\")\n",
    "    \n",
    "    # Mesurer le temps de chargement\n",
    "    start_time = datetime.now()\n",
    "    loaded_model = joblib.load(model_path)\n",
    "    end_time = datetime.now()\n",
    "    \n",
    "    loading_time = (end_time - start_time).total_seconds()\n",
    "    print(f\"Modèle chargé en {loading_time:.4f} secondes.\")\n",
    "    \n",
    "    # Afficher des informations sur le modèle\n",
    "    print(f\"Type du modèle: {type(loaded_model).__name__}\")\n",
    "    \n",
    "    if hasattr(loaded_model, 'feature_importances_'):\n",
    "        print(\"\\nImportance des caractéristiques:\")\n",
    "        importances = loaded_model.feature_importances_\n",
    "        \n",
    "        # Si le modèle est une pipeline, extraire le composant final\n",
    "        if hasattr(loaded_model, 'named_steps'):\n",
    "            if 'model' in loaded_model.named_steps:\n",
    "                importances = loaded_model.named_steps['model'].feature_importances_\n",
    "        \n",
    "        # Afficher les importances\n",
    "        for i, importance in enumerate(importances):\n",
    "            print(f\"  Caractéristique {i}: {importance:.4f}\")\n",
    "    \n",
    "    # Tester le modèle avec des données aléatoires\n",
    "    print(\"\\nTest du modèle avec des données aléatoires:\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    X_test = np.random.rand(5, loaded_model.n_features_in_)\n",
    "    \n",
    "    # Faire des prédictions\n",
    "    predictions = loaded_model.predict(X_test)\n",
    "    \n",
    "    # Afficher les prédictions\n",
    "    for i, pred in enumerate(predictions):\n",
    "        print(f\"  Échantillon {i}: {pred:.4f}\")\n",
    "    \n",
    "    # Taille du fichier modèle\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)  # en Mo\n",
    "    print(f\"\\nTaille du fichier modèle: {model_size:.2f} Mo\")\n",
    "    \n",
    "    # Afficher les métadonnées si disponibles\n",
    "    metadata_path = model_path.replace('.joblib', '_metadata.json')\n",
    "    if os.path.exists(metadata_path):\n",
    "        import json\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(\"\\nMétadonnées du modèle:\")\n",
    "        for key, value in metadata.items():\n",
    "            if key != 'performance':  # Afficher les performances séparément\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        if 'performance' in metadata:\n",
    "            print(\"\\nPerformance du modèle:\")\n",
    "            for metric, score in metadata['performance'].items():\n",
    "                if metric != 'best_params':  # Afficher les paramètres séparément\n",
    "                    print(f\"  {metric}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"Aucun modèle disponible dans le dossier des modèles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4e6017",
   "metadata": {},
   "source": [
    "## 6. Déploiement d'une API avec FastAPI\n",
    "\n",
    "Dans cette section, nous allons voir comment déployer un modèle entraîné via une API FastAPI. Nous allons :\n",
    "- Créer une application FastAPI\n",
    "- Définir des routes pour les prédictions\n",
    "- Tester l'API localement\n",
    "\n",
    "Notez que le code complet de l'API est déjà disponible dans le dossier `api/` du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f0fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examiner le code de l'API\n",
    "api_main_path = os.path.join(project_dir, 'api', 'main.py')\n",
    "\n",
    "if os.path.exists(api_main_path):\n",
    "    print(\"Aperçu du code de l'API FastAPI :\")\n",
    "    \n",
    "    with open(api_main_path, 'r') as f:\n",
    "        code = f.readlines()\n",
    "    \n",
    "    # Afficher les imports et l'initialisation de l'application\n",
    "    for i, line in enumerate(code):\n",
    "        if i < 30:  # Afficher les 30 premières lignes\n",
    "            print(line.rstrip())\n",
    "        elif i == 30:\n",
    "            print(\"...\")\n",
    "else:\n",
    "    print(\"Le fichier de l'API n'existe pas à l'emplacement attendu.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10129294",
   "metadata": {},
   "source": [
    "### 6.1 Exemple minimal d'API FastAPI\n",
    "\n",
    "Créons un exemple minimal d'API FastAPI pour servir un modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3449dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cet exemple montre comment créer une API FastAPI simple pour servir un modèle\n",
    "# Dans un environnement réel, vous exécuteriez ce code dans un fichier séparé\n",
    "\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Définir le modèle de données d'entrée\n",
    "class MovieInput(BaseModel):\n",
    "    \"\"\"Modèle pour les entrées de prédiction de film.\"\"\"\n",
    "    title: str = Field(..., example=\"The Shawshank Redemption\")\n",
    "    year: int = Field(..., example=1994)\n",
    "    director: str = Field(..., example=\"Frank Darabont\")\n",
    "    actors: str = Field(..., example=\"Tim Robbins, Morgan Freeman\")\n",
    "    genre: str = Field(..., example=\"Drama\")\n",
    "    plot: str = Field(..., example=\"Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.\")\n",
    "    runtime: int = Field(..., example=142)\n",
    "\n",
    "# Définir le modèle de données de sortie\n",
    "class PredictionResponse(BaseModel):\n",
    "    \"\"\"Modèle pour les réponses de prédiction.\"\"\"\n",
    "    prediction: float\n",
    "    confidence: float = None\n",
    "    model_info: Dict[str, Any]\n",
    "    features_used: List[str]\n",
    "    processing_time: float\n",
    "\n",
    "# Définir l'application FastAPI\n",
    "app = FastAPI(\n",
    "    title=\"Film Rating Predictor API\",\n",
    "    description=\"API pour prédire la note IMDb d'un film\",\n",
    "    version=\"1.0.0\"\n",
    ")\n",
    "\n",
    "# Charger le modèle (dans un environnement réel, vous pourriez le faire au démarrage de l'application)\n",
    "def load_model():\n",
    "    model_path = os.path.join(MODELS_DIR, 'demo_imdb_rating_predictor.joblib')\n",
    "    if os.path.exists(model_path):\n",
    "        return joblib.load(model_path)\n",
    "    else:\n",
    "        # Créer un modèle simple si aucun n'existe\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        X = np.random.rand(100, 5)\n",
    "        y = 2 * X[:, 0] + 3 * X[:, 1] - X[:, 2] + np.random.normal(0, 0.1, 100)\n",
    "        model.fit(X, y)\n",
    "        return model\n",
    "\n",
    "# Définir la route pour la page d'accueil\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Bienvenue sur l'API de prédiction de notes de films\"}\n",
    "\n",
    "# Définir la route pour les prédictions\n",
    "@app.post(\"/predict/rating\", response_model=PredictionResponse)\n",
    "async def predict_rating(movie: MovieInput):\n",
    "    try:\n",
    "        # Charger le modèle\n",
    "        model = load_model()\n",
    "        \n",
    "        # Dans un cas réel, vous effectueriez ici toutes les transformations nécessaires\n",
    "        # pour convertir les données d'entrée en format adapté au modèle\n",
    "        \n",
    "        # Simuler des caractéristiques extraites\n",
    "        features = np.array([\n",
    "            [\n",
    "                2023 - movie.year,               # age du film\n",
    "                len(movie.actors.split(',')),    # nombre d'acteurs\n",
    "                movie.runtime,                   # durée\n",
    "                len(movie.plot),                 # longueur du résumé\n",
    "                len(movie.genre.split(','))      # nombre de genres\n",
    "            ]\n",
    "        ])\n",
    "        \n",
    "        # Faire une prédiction\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        prediction = model.predict(features)[0]\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Préparer la réponse\n",
    "        return {\n",
    "            \"prediction\": float(prediction),\n",
    "            \"confidence\": 0.85,  # Valeur fictive pour l'exemple\n",
    "            \"model_info\": {\n",
    "                \"model_type\": \"RandomForestRegressor\",\n",
    "                \"version\": \"1.0\"\n",
    "            },\n",
    "            \"features_used\": [\"movie_age\", \"actor_count\", \"runtime\", \"plot_length\", \"genre_count\"],\n",
    "            \"processing_time\": end_time - start_time\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Erreur lors de la prédiction: {str(e)}\")\n",
    "\n",
    "# Afficher un message d'information (dans un environnement réel, vous lanceriez l'API avec uvicorn)\n",
    "print(\"Dans un environnement réel, vous lanceriez l'API avec la commande :\")\n",
    "print(\"uvicorn main:app --reload\")\n",
    "print(\"\\nL'API serait alors accessible à l'adresse : http://localhost:8000\")\n",
    "print(\"La documentation Swagger serait disponible à : http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7f6f21",
   "metadata": {},
   "source": [
    "### 6.2 Simulation de requêtes à l'API\n",
    "\n",
    "Simulons des requêtes à notre API (sans avoir besoin de la démarrer réellement) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13246bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simuler des requêtes à l'API en appelant directement les fonctions\n",
    "\n",
    "# Créer quelques exemples de films\n",
    "sample_movies = [\n",
    "    MovieInput(\n",
    "        title=\"The Shawshank Redemption\",\n",
    "        year=1994,\n",
    "        director=\"Frank Darabont\",\n",
    "        actors=\"Tim Robbins, Morgan Freeman\",\n",
    "        genre=\"Drama\",\n",
    "        plot=\"Two imprisoned men bond over a number of years, finding solace and eventual redemption through acts of common decency.\",\n",
    "        runtime=142\n",
    "    ),\n",
    "    MovieInput(\n",
    "        title=\"Inception\",\n",
    "        year=2010,\n",
    "        director=\"Christopher Nolan\",\n",
    "        actors=\"Leonardo DiCaprio, Joseph Gordon-Levitt, Ellen Page\",\n",
    "        genre=\"Action, Adventure, Sci-Fi\",\n",
    "        plot=\"A thief who steals corporate secrets through the use of dream-sharing technology is given the inverse task of planting an idea into the mind of a C.E.O.\",\n",
    "        runtime=148\n",
    "    ),\n",
    "    MovieInput(\n",
    "        title=\"The Godfather\",\n",
    "        year=1972,\n",
    "        director=\"Francis Ford Coppola\",\n",
    "        actors=\"Marlon Brando, Al Pacino, James Caan\",\n",
    "        genre=\"Crime, Drama\",\n",
    "        plot=\"The aging patriarch of an organized crime dynasty transfers control of his clandestine empire to his reluctant son.\",\n",
    "        runtime=175\n",
    "    )\n",
    "]\n",
    "\n",
    "# Simuler des prédictions pour chaque film\n",
    "for i, movie in enumerate(sample_movies):\n",
    "    print(f\"\\n--- Film {i+1}: {movie.title} ({movie.year}) ---\")\n",
    "    \n",
    "    # Appeler directement la fonction de prédiction\n",
    "    response = await predict_rating(movie)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    print(f\"Note prédite: {response['prediction']:.2f}/10\")\n",
    "    print(f\"Confiance: {response['confidence']:.2f}\")\n",
    "    print(f\"Temps de traitement: {response['processing_time']*1000:.2f} ms\")\n",
    "    print(f\"Caractéristiques utilisées: {', '.join(response['features_used'])}\")\n",
    "\n",
    "# Créer un graphique comparant les notes prédites\n",
    "predictions = []\n",
    "titles = []\n",
    "\n",
    "for movie in sample_movies:\n",
    "    response = await predict_rating(movie)\n",
    "    predictions.append(response['prediction'])\n",
    "    titles.append(f\"{movie.title} ({movie.year})\")\n",
    "\n",
    "# Visualiser les prédictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(titles, predictions)\n",
    "plt.ylim(0, 10)\n",
    "plt.xlabel('Film')\n",
    "plt.ylabel('Note IMDb prédite')\n",
    "plt.title('Prédictions de notes IMDb pour différents films')\n",
    "\n",
    "# Ajouter les valeurs au-dessus des barres\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49ae06",
   "metadata": {},
   "source": [
    "## 7. Optimisation des modèles avec Cross-Validation et Hyperparameter Tuning\n",
    "\n",
    "Dans cette section, nous allons explorer comment optimiser les modèles de machine learning en utilisant la validation croisée (cross-validation) et l'optimisation des hyperparamètres. Ces techniques sont essentielles pour éviter le surapprentissage et obtenir les meilleures performances possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f13983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Créer un dataset synthétique pour l'exemple\n",
    "np.random.seed(42)\n",
    "n_samples = 200\n",
    "X = np.random.rand(n_samples, 5)\n",
    "# Créer une variable cible avec du bruit\n",
    "y = 5 + 2*X[:, 0] + 3*X[:, 1] - 1.5*X[:, 2] + 0.5*X[:, 3] + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Convertir en DataFrame pour une meilleure lisibilité\n",
    "feature_names = ['film_age', 'num_awards', 'runtime_min', 'budget_millions', 'marketing_score']\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "print(\"Jeu de données synthétique créé pour la démonstration :\")\n",
    "print(f\"Nombre d'échantillons: {n_samples}\")\n",
    "print(f\"Caractéristiques: {', '.join(feature_names)}\")\n",
    "\n",
    "# Visualiser les relations entre les caractéristiques et la cible\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(X_df[feature], y, alpha=0.5)\n",
    "    plt.title(f'Relation entre {feature} et la note')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Note')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Validation croisée sur un modèle de base\n",
    "print(\"\\n1. Validation croisée sur un modèle de base\")\n",
    "base_model = RandomForestRegressor(random_state=42)\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Calculer les scores de validation croisée\n",
    "cv_scores = cross_val_score(base_model, X, y, cv=cv, scoring='r2')\n",
    "print(f\"Scores R² par fold: {cv_scores}\")\n",
    "print(f\"Score R² moyen: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "\n",
    "# Création d'un pipeline pour prétraitement + modélisation\n",
    "print(\"\\n2. Création d'un pipeline avec prétraitement\")\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Scores avec le pipeline\n",
    "pipeline_cv_scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')\n",
    "print(f\"Scores R² du pipeline par fold: {pipeline_cv_scores}\")\n",
    "print(f\"Score R² moyen du pipeline: {pipeline_cv_scores.mean():.4f} ± {pipeline_cv_scores.std():.4f}\")\n",
    "\n",
    "# Optimisation des hyperparamètres avec GridSearchCV\n",
    "print(\"\\n3. Optimisation des hyperparamètres avec GridSearchCV\")\n",
    "print(\"Cette opération peut prendre un peu de temps...\")\n",
    "\n",
    "# Définir les paramètres à tester\n",
    "param_grid = {\n",
    "    'model__n_estimators': [50, 100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Créer le GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline, \n",
    "    param_grid, \n",
    "    cv=cv, \n",
    "    scoring='r2', \n",
    "    return_train_score=True,\n",
    "    n_jobs=-1  # Utiliser tous les cœurs disponibles\n",
    ")\n",
    "\n",
    "# Mesurer le temps d'exécution\n",
    "start_time = time.time()\n",
    "grid_search.fit(X, y)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Temps d'exécution: {end_time - start_time:.2f} secondes\")\n",
    "\n",
    "# Afficher les meilleurs paramètres\n",
    "print(f\"\\nMeilleurs paramètres: {grid_search.best_params_}\")\n",
    "print(f\"Meilleur score R²: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Créer un DataFrame des résultats pour une analyse plus facile\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Sélectionner les colonnes pertinentes\n",
    "cols = ['param_model__n_estimators', 'param_model__max_depth', \n",
    "        'param_model__min_samples_split', 'mean_test_score', \n",
    "        'std_test_score', 'mean_train_score', 'std_train_score', 'rank_test_score']\n",
    "results_df = results[cols].sort_values('rank_test_score')\n",
    "\n",
    "# Afficher les 5 meilleures combinaisons\n",
    "print(\"\\nTop 5 des combinaisons de paramètres:\")\n",
    "display(results_df.head())\n",
    "\n",
    "# Visualiser l'impact des paramètres sur la performance\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Impact du nombre d'arbres\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='param_model__n_estimators', y='mean_test_score', data=results)\n",
    "plt.title('Impact du nombre d\\'arbres sur le score R²')\n",
    "plt.xlabel('Nombre d\\'arbres')\n",
    "plt.ylabel('Score R²')\n",
    "\n",
    "# Impact de la profondeur maximale\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(x='param_model__max_depth', y='mean_test_score', data=results)\n",
    "plt.title('Impact de la profondeur maximale sur le score R²')\n",
    "plt.xlabel('Profondeur maximale')\n",
    "plt.ylabel('Score R²')\n",
    "\n",
    "# Impact du nombre minimum d'échantillons pour diviser\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(x='param_model__min_samples_split', y='mean_test_score', data=results)\n",
    "plt.title('Impact du min_samples_split sur le score R²')\n",
    "plt.xlabel('Valeur de min_samples_split')\n",
    "plt.ylabel('Score R²')\n",
    "\n",
    "# Comparer train vs test pour détecter le surapprentissage\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(results['mean_train_score'], results['mean_test_score'], alpha=0.7)\n",
    "plt.plot([0.5, 1], [0.5, 1], 'r--')  # Ligne d'égalité\n",
    "plt.xlim(0.8, 1)\n",
    "plt.ylim(0.8, 1)\n",
    "plt.title('Train vs Test Scores')\n",
    "plt.xlabel('Score R² moyen sur train')\n",
    "plt.ylabel('Score R² moyen sur test')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Évaluer le meilleur modèle\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\nPerformance du meilleur modèle par validation croisée:\")\n",
    "best_cv_scores = cross_val_score(best_model, X, y, cv=cv, scoring='r2')\n",
    "print(f\"Scores R² par fold: {best_cv_scores}\")\n",
    "print(f\"Score R² moyen: {best_cv_scores.mean():.4f} ± {best_cv_scores.std():.4f}\")\n",
    "\n",
    "# Comparer différents algorithmes\n",
    "print(\"\\n4. Comparaison de différents algorithmes\")\n",
    "\n",
    "models = {\n",
    "    'Ridge': Ridge(random_state=42),\n",
    "    'RandomForest': RandomForestRegressor(random_state=42),\n",
    "    'GradientBoosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    scores = cross_val_score(pipeline, X, y, cv=cv, scoring='r2')\n",
    "    model_results[name] = {\n",
    "        'scores': scores,\n",
    "        'mean': scores.mean(),\n",
    "        'std': scores.std()\n",
    "    }\n",
    "    print(f\"{name}: R² = {scores.mean():.4f} ± {scores.std():.4f}\")\n",
    "\n",
    "# Visualiser la comparaison des modèles\n",
    "plt.figure(figsize=(10, 6))\n",
    "model_names = list(model_results.keys())\n",
    "means = [model_results[name]['mean'] for name in model_names]\n",
    "stds = [model_results[name]['std'] for name in model_names]\n",
    "\n",
    "bars = plt.bar(model_names, means, yerr=stds, capsize=10, alpha=0.7)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.axhline(y=0.9, color='r', linestyle='--', alpha=0.7)\n",
    "plt.title('Comparaison des performances des modèles (R²)')\n",
    "plt.ylabel('Score R²')\n",
    "plt.grid(True, linestyle='--', alpha=0.7, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusion sur l'optimisation\n",
    "print(\"\\nConclusion sur l'optimisation des modèles:\")\n",
    "print(\"1. La validation croisée permet d'obtenir une estimation plus robuste des performances\")\n",
    "print(\"2. L'optimisation des hyperparamètres permet d'améliorer significativement les performances\")\n",
    "print(\"3. Le choix de l'algorithme a un impact important sur les résultats\")\n",
    "print(\"4. Il est important de surveiller le surapprentissage en comparant les scores train et test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c16e23",
   "metadata": {},
   "source": [
    "## 8. Interprétabilité et Explainabilité des Modèles\n",
    "\n",
    "L'interprétabilité des modèles est essentielle pour comprendre les prédictions et gagner la confiance des utilisateurs. Dans cette section, nous allons explorer des techniques pour expliquer les prédictions des modèles de machine learning, notamment à l'aide de SHAP (SHapley Additive exPlanations) et d'autres méthodes d'interprétation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05561c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Note: Normalement, nous utiliserions également SHAP, mais nous allons simuler\n",
    "# son comportement pour éviter l'installation dans ce notebook\n",
    "\n",
    "# Créer un dataset simple pour l'exemple\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X = np.random.rand(n_samples, 5)\n",
    "feature_names = ['film_age', 'num_awards', 'runtime_min', 'budget_millions', 'marketing_score']\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Créer une relation connue entre les caractéristiques et la cible\n",
    "# avec des coefficients d'importance différents\n",
    "coefficients = [2.5, 1.8, -0.5, 3.2, 0.9]\n",
    "y = np.dot(X, coefficients) + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "print(\"Coefficients réels d'importance des caractéristiques:\")\n",
    "for feature, coef in zip(feature_names, coefficients):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "# Entraîner un modèle de forêt aléatoire\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_df, y)\n",
    "\n",
    "print(\"\\n1. Importance des caractéristiques basée sur l'impureté (méthode intégrée)\")\n",
    "# Extraire et afficher l'importance des caractéristiques\n",
    "feature_importances = model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "display(importance_df)\n",
    "\n",
    "# Visualiser l'importance des caractéristiques\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Importance des caractéristiques (méthode intégrée)')\n",
    "plt.gca().invert_yaxis()  # Pour avoir la caractéristique la plus importante en haut\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2. Importance des caractéristiques par permutation\")\n",
    "# Calculer l'importance par permutation\n",
    "perm_importance = permutation_importance(model, X_df, y, n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean,\n",
    "    'Std': perm_importance.importances_std\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "display(perm_importance_df)\n",
    "\n",
    "# Visualiser l'importance par permutation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(perm_importance_df['Feature'], perm_importance_df['Importance'], \n",
    "         xerr=perm_importance_df['Std'], capsize=5)\n",
    "plt.xlabel('Importance (diminution moyenne de la performance)')\n",
    "plt.title('Importance des caractéristiques par permutation')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n3. Graphiques de dépendance partielle (simulation)\")\n",
    "# Simuler des graphiques de dépendance partielle\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Créer un ensemble de valeurs pour la caractéristique\n",
    "    feature_values = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Créer un jeu de données où toutes les autres caractéristiques sont à leur moyenne\n",
    "    X_mean = np.tile(X_df.mean().values, (100, 1))\n",
    "    \n",
    "    # Remplacer la caractéristique d'intérêt par les valeurs générées\n",
    "    X_mean[:, i] = feature_values\n",
    "    \n",
    "    # Faire des prédictions\n",
    "    predictions = model.predict(X_mean)\n",
    "    \n",
    "    # Tracer la dépendance partielle\n",
    "    plt.plot(feature_values, predictions)\n",
    "    plt.title(f'Dépendance partielle: {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Prédiction moyenne')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n4. Simulation des valeurs SHAP\")\n",
    "# Simuler des valeurs SHAP pour un exemple\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sélectionner un exemple aléatoire\n",
    "sample_idx = np.random.randint(0, n_samples)\n",
    "sample = X_df.iloc[sample_idx]\n",
    "prediction = model.predict([sample])[0]\n",
    "\n",
    "print(f\"Exemple sélectionné (index {sample_idx}):\")\n",
    "for feature, value in sample.items():\n",
    "    print(f\"{feature}: {value:.4f}\")\n",
    "print(f\"Prédiction: {prediction:.4f}\")\n",
    "\n",
    "# Simuler des valeurs SHAP\n",
    "# Note: Dans un cas réel, nous utiliserions:\n",
    "# import shap\n",
    "# explainer = shap.TreeExplainer(model)\n",
    "# shap_values = explainer.shap_values(sample)\n",
    "\n",
    "# Simuler des valeurs SHAP basées sur l'importance des caractéristiques\n",
    "base_value = model.predict(np.mean(X_df, axis=0).reshape(1, -1))[0]\n",
    "contributions = []\n",
    "\n",
    "for i, (feature, value) in enumerate(sample.items()):\n",
    "    # Simuler une contribution basée sur l'écart par rapport à la moyenne et l'importance\n",
    "    contribution = (value - X_df[feature].mean()) * feature_importances[i] * 5\n",
    "    contributions.append(contribution)\n",
    "\n",
    "# Normaliser pour que la somme corresponde à l'écart de prédiction\n",
    "contributions = np.array(contributions)\n",
    "scale_factor = (prediction - base_value) / contributions.sum()\n",
    "shap_values = contributions * scale_factor\n",
    "\n",
    "# Créer un DataFrame des valeurs SHAP\n",
    "shap_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'SHAP Value': shap_values,\n",
    "    'Feature Value': sample.values\n",
    "}).sort_values('SHAP Value', ascending=False)\n",
    "\n",
    "print(\"\\nValeurs SHAP simulées:\")\n",
    "display(shap_df)\n",
    "\n",
    "# Visualiser les valeurs SHAP\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['red' if x > 0 else 'blue' for x in shap_df['SHAP Value']]\n",
    "plt.barh(shap_df['Feature'], shap_df['SHAP Value'], color=colors)\n",
    "plt.axvline(x=0, color='gray', linestyle='--')\n",
    "plt.xlabel('Impact sur la prédiction (valeur SHAP)')\n",
    "plt.title(f'Explication de la prédiction (valeur de base: {base_value:.4f}, prédiction: {prediction:.4f})')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n5. Force plot simulé\")\n",
    "# Créer un graphique de force simulé pour montrer l'impact cumulatif\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Trier les valeurs SHAP par ordre absolu décroissant\n",
    "shap_order = np.argsort(np.abs(shap_values))[::-1]\n",
    "ordered_features = [feature_names[i] for i in shap_order]\n",
    "ordered_shap = shap_values[shap_order]\n",
    "\n",
    "# Calculer les valeurs cumulatives\n",
    "cumulative = np.cumsum(ordered_shap) + base_value\n",
    "all_values = np.concatenate(([base_value], cumulative))\n",
    "\n",
    "# Tracer le graphique en cascade\n",
    "plt.plot([0, len(ordered_features) + 1], [base_value, prediction], 'k--', alpha=0.5)\n",
    "plt.scatter(range(1, len(ordered_features) + 1), cumulative, s=50)\n",
    "\n",
    "# Ajouter des annotations\n",
    "for i, (feature, shap_val) in enumerate(zip(ordered_features, ordered_shap)):\n",
    "    x = i + 1\n",
    "    y = all_values[i]\n",
    "    direction = 'up' if shap_val > 0 else 'down'\n",
    "    color = 'green' if shap_val > 0 else 'red'\n",
    "    plt.annotate(\n",
    "        f\"{feature} ({shap_val:.2f})\",\n",
    "        xy=(x, all_values[i+1]),\n",
    "        xytext=(x, all_values[i+1] + (0.1 if direction == 'up' else -0.1)),\n",
    "        arrowprops=dict(arrowstyle='->', color=color),\n",
    "        ha='center'\n",
    "    )\n",
    "    plt.plot([x, x], [y, all_values[i+1]], color=color)\n",
    "\n",
    "plt.xticks([0] + list(range(1, len(ordered_features) + 1)), ['base'] + ordered_features, rotation=45, ha='right')\n",
    "plt.ylabel('Prédiction')\n",
    "plt.title('Force Plot: Impact cumulatif des caractéristiques sur la prédiction')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCette section illustre différentes méthodes pour interpréter les modèles :\")\n",
    "print(\"1. Importance des caractéristiques basée sur l'impureté (intégrée aux arbres de décision)\")\n",
    "print(\"2. Importance par permutation (plus robuste car basée sur la dégradation des performances)\")\n",
    "print(\"3. Graphiques de dépendance partielle (montrent comment une caractéristique influence la prédiction)\")\n",
    "print(\"4. Valeurs SHAP (attribuent une contribution à chaque caractéristique pour une prédiction)\")\n",
    "print(\"5. Force plots (visualisent l'impact cumulatif des caractéristiques)\")\n",
    "print(\"\\nDans un environnement réel, il serait recommandé d'utiliser la bibliothèque SHAP pour des explications plus précises.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea90c75a",
   "metadata": {},
   "source": [
    "## 9. Surveillance et Évaluation Continue\n",
    "\n",
    "Une fois un modèle déployé, il est crucial de le surveiller et de l'évaluer en continu pour s'assurer qu'il fonctionne correctement et que ses performances ne se dégradent pas au fil du temps. Dans cette section, nous allons explorer quelques techniques pour surveiller et évaluer les modèles en production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41942de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer les bibliothèques nécessaires\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Simulation d'un système de surveillance et d'évaluation continue des modèles\")\n",
    "\n",
    "# Créer un dataset synthétique pour l'exemple\n",
    "np.random.seed(42)\n",
    "n_samples = 500\n",
    "X = np.random.rand(n_samples, 5)\n",
    "feature_names = ['film_age', 'num_awards', 'runtime_min', 'budget_millions', 'marketing_score']\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "\n",
    "# Créer une relation connue entre les caractéristiques et la cible\n",
    "y = 5 + 2*X[:, 0] + 3*X[:, 1] - 1.5*X[:, 2] + 0.5*X[:, 3] + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "# Division en train/test chronologique (simulé)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_df, y, test_size=0.4, shuffle=False)\n",
    "\n",
    "print(f\"Données d'entraînement: {X_train.shape[0]} exemples\")\n",
    "print(f\"Données de test: {X_test.shape[0]} exemples\")\n",
    "\n",
    "# Entraîner un modèle initial\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n1. Surveillance des performances sur le temps\")\n",
    "print(\"Simulation d'une évaluation sur 10 semaines...\")\n",
    "\n",
    "# Diviser les données de test en 10 semaines\n",
    "test_splits = np.array_split(list(range(len(X_test))), 10)\n",
    "weeks = range(1, 11)\n",
    "weekly_metrics = []\n",
    "\n",
    "# Simuler un drift graduel dans les données\n",
    "drift_factor = np.linspace(0, 0.5, 10)  # Augmentation graduelle du drift\n",
    "\n",
    "for week, split, drift in zip(weeks, test_splits, drift_factor):\n",
    "    X_week = X_test.iloc[split]\n",
    "    y_week = y_test.iloc[split]\n",
    "    \n",
    "    # Appliquer un drift simulé aux caractéristiques\n",
    "    X_week_drift = X_week.copy()\n",
    "    X_week_drift['marketing_score'] = X_week_drift['marketing_score'] + drift\n",
    "    \n",
    "    # Faire des prédictions\n",
    "    y_pred = model.predict(X_week_drift)\n",
    "    \n",
    "    # Calculer les métriques\n",
    "    rmse = np.sqrt(mean_squared_error(y_week, y_pred))\n",
    "    mae = mean_absolute_error(y_week, y_pred)\n",
    "    r2 = r2_score(y_week, y_pred)\n",
    "    \n",
    "    weekly_metrics.append({\n",
    "        'Week': week,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'Drift': drift\n",
    "    })\n",
    "\n",
    "# Convertir en DataFrame\n",
    "metrics_df = pd.DataFrame(weekly_metrics)\n",
    "display(metrics_df)\n",
    "\n",
    "# Visualiser l'évolution des métriques\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# RMSE au fil du temps\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(metrics_df['Week'], metrics_df['RMSE'], 'o-', linewidth=2)\n",
    "plt.axhline(y=np.mean(metrics_df['RMSE']), color='r', linestyle='--', alpha=0.7)\n",
    "plt.fill_between(\n",
    "    metrics_df['Week'], \n",
    "    np.mean(metrics_df['RMSE']) - np.std(metrics_df['RMSE']), \n",
    "    np.mean(metrics_df['RMSE']) + np.std(metrics_df['RMSE']), \n",
    "    alpha=0.2, color='r'\n",
    ")\n",
    "plt.title('RMSE au fil du temps')\n",
    "plt.xlabel('Semaine')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True)\n",
    "\n",
    "# MAE au fil du temps\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(metrics_df['Week'], metrics_df['MAE'], 'o-', linewidth=2, color='green')\n",
    "plt.axhline(y=np.mean(metrics_df['MAE']), color='r', linestyle='--', alpha=0.7)\n",
    "plt.fill_between(\n",
    "    metrics_df['Week'], \n",
    "    np.mean(metrics_df['MAE']) - np.std(metrics_df['MAE']), \n",
    "    np.mean(metrics_df['MAE']) + np.std(metrics_df['MAE']), \n",
    "    alpha=0.2, color='r'\n",
    ")\n",
    "plt.title('MAE au fil du temps')\n",
    "plt.xlabel('Semaine')\n",
    "plt.ylabel('MAE')\n",
    "plt.grid(True)\n",
    "\n",
    "# R² au fil du temps\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(metrics_df['Week'], metrics_df['R2'], 'o-', linewidth=2, color='purple')\n",
    "plt.axhline(y=np.mean(metrics_df['R2']), color='r', linestyle='--', alpha=0.7)\n",
    "plt.fill_between(\n",
    "    metrics_df['Week'], \n",
    "    np.mean(metrics_df['R2']) - np.std(metrics_df['R2']), \n",
    "    np.mean(metrics_df['R2']) + np.std(metrics_df['R2']), \n",
    "    alpha=0.2, color='r'\n",
    ")\n",
    "plt.title('R² au fil du temps')\n",
    "plt.xlabel('Semaine')\n",
    "plt.ylabel('R²')\n",
    "plt.grid(True)\n",
    "\n",
    "# Relation entre le drift et l'erreur\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.scatter(metrics_df['Drift'], metrics_df['RMSE'], alpha=0.7, s=100)\n",
    "plt.title('Impact du Drift sur l\\'erreur')\n",
    "plt.xlabel('Facteur de Drift')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n2. Simulation d'une détection de drift de données\")\n",
    "\n",
    "# Créer des données avec drift pour la simulation\n",
    "np.random.seed(43)\n",
    "n_drift_samples = 200\n",
    "X_new = np.random.rand(n_drift_samples, 5) * 1.2 - 0.1  # Légèrement décalé\n",
    "X_new_df = pd.DataFrame(X_new, columns=feature_names)\n",
    "\n",
    "# Analyser la distribution des caractéristiques\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feature in enumerate(feature_names):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    \n",
    "    # Distributions originales et nouvelles\n",
    "    sns.kdeplot(X_df[feature], label='Original', alpha=0.7)\n",
    "    sns.kdeplot(X_new_df[feature], label='Nouveaux', alpha=0.7)\n",
    "    \n",
    "    plt.title(f'Distribution de {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calcul de la divergence entre distributions (approximation simple)\n",
    "def distribution_divergence(dist1, dist2, bins=20):\n",
    "    \"\"\"Calcule une approximation simple de la divergence entre deux distributions.\"\"\"\n",
    "    hist1, edges = np.histogram(dist1, bins=bins, density=True)\n",
    "    hist2, _ = np.histogram(dist2, bins=edges, density=True)\n",
    "    \n",
    "    # Éviter la division par zéro\n",
    "    hist1 = np.maximum(hist1, 1e-10)\n",
    "    hist2 = np.maximum(hist2, 1e-10)\n",
    "    \n",
    "    # Approximation de la divergence KL\n",
    "    kl_div = np.sum(hist1 * np.log(hist1 / hist2))\n",
    "    return kl_div\n",
    "\n",
    "# Calculer la divergence pour chaque caractéristique\n",
    "divergences = {}\n",
    "for feature in feature_names:\n",
    "    div = distribution_divergence(X_df[feature], X_new_df[feature])\n",
    "    divergences[feature] = div\n",
    "\n",
    "# Afficher les divergences\n",
    "divergences_df = pd.DataFrame({\n",
    "    'Feature': list(divergences.keys()),\n",
    "    'Divergence': list(divergences.values())\n",
    "}).sort_values('Divergence', ascending=False)\n",
    "\n",
    "print(\"Divergence entre les distributions originales et nouvelles:\")\n",
    "display(divergences_df)\n",
    "\n",
    "# Visualiser les divergences\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(divergences_df['Feature'], divergences_df['Divergence'])\n",
    "plt.title('Divergence entre les distributions originales et nouvelles')\n",
    "plt.ylabel('Divergence (approx. KL)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', label='Seuil d\\'alerte')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n3. Simulation d'un système de surveillance des performances\")\n",
    "\n",
    "# Simuler un tableau de bord de surveillance\n",
    "print(\"Tableau de bord de surveillance des modèles\")\n",
    "print(\"------------------------------------------\")\n",
    "\n",
    "# Métriques générales\n",
    "avg_rmse = metrics_df['RMSE'].mean()\n",
    "last_rmse = metrics_df['RMSE'].iloc[-1]\n",
    "rmse_change = (last_rmse - avg_rmse) / avg_rmse * 100\n",
    "\n",
    "avg_r2 = metrics_df['R2'].mean()\n",
    "last_r2 = metrics_df['R2'].iloc[-1]\n",
    "r2_change = (last_r2 - avg_r2) / avg_r2 * 100\n",
    "\n",
    "# Afficher les métriques principales\n",
    "print(f\"RMSE moyenne: {avg_rmse:.4f}\")\n",
    "print(f\"RMSE actuelle: {last_rmse:.4f} ({'+' if rmse_change > 0 else ''}{rmse_change:.2f}%)\")\n",
    "print(f\"R² moyen: {avg_r2:.4f}\")\n",
    "print(f\"R² actuel: {last_r2:.4f} ({'+' if r2_change > 0 else ''}{r2_change:.2f}%)\")\n",
    "\n",
    "# Statut du modèle\n",
    "if rmse_change > 10 or r2_change < -10:\n",
    "    status = \"ALERTE\"\n",
    "elif rmse_change > 5 or r2_change < -5:\n",
    "    status = \"AVERTISSEMENT\"\n",
    "else:\n",
    "    status = \"NORMAL\"\n",
    "\n",
    "print(f\"Statut du modèle: {status}\")\n",
    "\n",
    "# Recommandations\n",
    "print(\"\\nRecommandations:\")\n",
    "if status == \"ALERTE\":\n",
    "    print(\"- Réentraîner le modèle immédiatement avec les données récentes\")\n",
    "    print(\"- Vérifier la qualité des données entrantes\")\n",
    "    print(\"- Revoir les caractéristiques impactées par le drift\")\n",
    "elif status == \"AVERTISSEMENT\":\n",
    "    print(\"- Prévoir un réentraînement dans la semaine\")\n",
    "    print(\"- Surveiller de près les caractéristiques avec drift élevé\")\n",
    "    print(\"- Analyser les prédictions erronées\")\n",
    "else:\n",
    "    print(\"- Continuer la surveillance régulière\")\n",
    "    print(\"- Planifier le réentraînement mensuel standard\")\n",
    "\n",
    "# Simuler un historique des réentraînements\n",
    "print(\"\\nHistorique des réentraînements:\")\n",
    "today = datetime.now()\n",
    "\n",
    "retraining_history = [\n",
    "    {\"date\": (today - timedelta(days=60)).strftime(\"%Y-%m-%d\"), \"version\": \"1.0\", \"reason\": \"Initial deployment\"},\n",
    "    {\"date\": (today - timedelta(days=45)).strftime(\"%Y-%m-%d\"), \"version\": \"1.1\", \"reason\": \"Scheduled update\"},\n",
    "    {\"date\": (today - timedelta(days=30)).strftime(\"%Y-%m-%d\"), \"version\": \"1.2\", \"reason\": \"Feature improvement\"},\n",
    "    {\"date\": (today - timedelta(days=15)).strftime(\"%Y-%m-%d\"), \"version\": \"1.3\", \"reason\": \"Data drift correction\"}\n",
    "]\n",
    "\n",
    "for entry in retraining_history:\n",
    "    print(f\"- {entry['date']}: v{entry['version']} - {entry['reason']}\")\n",
    "\n",
    "print(\"\\nProchain réentraînement prévu: \" + (today + timedelta(days=15)).strftime(\"%Y-%m-%d\"))\n",
    "\n",
    "print(\"\\nCette section a démontré :\")\n",
    "print(\"1. Comment surveiller les performances d'un modèle au fil du temps\")\n",
    "print(\"2. Comment détecter et mesurer le drift dans les données\")\n",
    "print(\"3. Comment implémenter un système d'alerte pour la dégradation des performances\")\n",
    "print(\"4. L'importance de la planification des réentraînements réguliers\")\n",
    "print(\"\\nEn production, ces mécanismes seraient automatisés et intégrés à un pipeline de MLOps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a2182d",
   "metadata": {},
   "source": [
    "## 10. Conclusion et Bonnes Pratiques\n",
    "\n",
    "Dans ce notebook, nous avons couvert l'ensemble du pipeline de data engineering et de machine learning, de la collecte des données au déploiement d'une API. Voici un récapitulatif des étapes et des bonnes pratiques pour chacune d'entre elles :\n",
    "\n",
    "### 1. Collecte de données\n",
    "- Utiliser différentes sources (APIs, webscraping, fichiers locaux)\n",
    "- Implémenter une gestion des erreurs robuste\n",
    "- Documenter la provenance des données\n",
    "\n",
    "### 2. Nettoyage des données\n",
    "- Traiter les valeurs manquantes de manière adaptée au contexte\n",
    "- Normaliser les formats de données\n",
    "- Détecter et gérer les outliers\n",
    "- Effectuer des vérifications de cohérence\n",
    "\n",
    "### 3. Feature Engineering\n",
    "- Créer des caractéristiques pertinentes pour le problème\n",
    "- Encoder les variables catégorielles\n",
    "- Normaliser/standardiser les variables numériques\n",
    "- Extraire des caractéristiques textuelles (sentiments, TF-IDF, etc.)\n",
    "\n",
    "### 4. Entraînement de modèles\n",
    "- Tester différents algorithmes\n",
    "- Optimiser les hyperparamètres\n",
    "- Utiliser la validation croisée\n",
    "- Évaluer avec des métriques adaptées\n",
    "\n",
    "### 5. Stockage et chargement de modèles\n",
    "- Utiliser des formats standardisés (joblib, pickle)\n",
    "- Stocker les métadonnées (performances, paramètres, etc.)\n",
    "- Versionner les modèles\n",
    "\n",
    "### 6. Déploiement d'API\n",
    "- Utiliser des frameworks modernes (FastAPI)\n",
    "- Documenter l'API (Swagger, OpenAPI)\n",
    "- Gérer les erreurs\n",
    "- Optimiser les performances\n",
    "\n",
    "### 7. Optimisation des modèles\n",
    "- Utiliser la validation croisée\n",
    "- Appliquer l'optimisation des hyperparamètres\n",
    "- Surveiller le surapprentissage\n",
    "\n",
    "### 8. Interprétabilité des modèles\n",
    "- Analyser l'importance des caractéristiques\n",
    "- Utiliser des méthodes d'explicabilité (SHAP, LIME)\n",
    "- Visualiser les dépendances partielles\n",
    "\n",
    "### 9. Surveillance et évaluation continue\n",
    "- Monitorer les performances au fil du temps\n",
    "- Détecter les drifts de données\n",
    "- Planifier des réentraînements réguliers\n",
    "\n",
    "Ce pipeline complet illustre les étapes essentielles pour développer et déployer des modèles de machine learning en production. Il peut être adapté à différents cas d'usage et étendu selon les besoins spécifiques.\n",
    "\n",
    "La mise en place d'un tel pipeline nécessite une bonne compréhension des concepts de data engineering et de machine learning, ainsi qu'une maîtrise des outils et technologies associés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1985fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher quelques ressources supplémentaires et prochaines étapes\n",
    "\n",
    "print(\"Ressources pour approfondir :\")\n",
    "print(\"-----------------------------\")\n",
    "\n",
    "resources = [\n",
    "    {\"topic\": \"Data Engineering\", \"resources\": [\n",
    "        \"Apache Airflow - https://airflow.apache.org/\",\n",
    "        \"dbt (data build tool) - https://www.getdbt.com/\",\n",
    "        \"Great Expectations - https://greatexpectations.io/\"\n",
    "    ]},\n",
    "    {\"topic\": \"Machine Learning\", \"resources\": [\n",
    "        \"scikit-learn - https://scikit-learn.org/\",\n",
    "        \"XGBoost - https://xgboost.readthedocs.io/\",\n",
    "        \"TensorFlow - https://www.tensorflow.org/\"\n",
    "    ]},\n",
    "    {\"topic\": \"MLOps\", \"resources\": [\n",
    "        \"MLflow - https://mlflow.org/\",\n",
    "        \"DVC - https://dvc.org/\",\n",
    "        \"Kubeflow - https://www.kubeflow.org/\"\n",
    "    ]},\n",
    "    {\"topic\": \"Explainabilité\", \"resources\": [\n",
    "        \"SHAP - https://github.com/slundberg/shap\",\n",
    "        \"LIME - https://github.com/marcotcr/lime\",\n",
    "        \"ELI5 - https://eli5.readthedocs.io/\"\n",
    "    ]},\n",
    "    {\"topic\": \"API & Déploiement\", \"resources\": [\n",
    "        \"FastAPI - https://fastapi.tiangolo.com/\",\n",
    "        \"Docker - https://www.docker.com/\",\n",
    "        \"Kubernetes - https://kubernetes.io/\"\n",
    "    ]}\n",
    "]\n",
    "\n",
    "for category in resources:\n",
    "    print(f\"\\n{category['topic']}:\")\n",
    "    for resource in category['resources']:\n",
    "        print(f\"- {resource}\")\n",
    "\n",
    "print(\"\\nProchaines étapes possibles pour ce projet :\")\n",
    "print(\"-------------------------------------------\")\n",
    "next_steps = [\n",
    "    \"Implémenter des tests unitaires pour chaque composant du pipeline\",\n",
    "    \"Ajouter un système de versionnage des données avec DVC\",\n",
    "    \"Automatiser le pipeline complet avec Apache Airflow\",\n",
    "    \"Conteneuriser l'application avec Docker\",\n",
    "    \"Implémenter un système de feedback pour améliorer le modèle\",\n",
    "    \"Ajouter des méthodes d'explainabilité avancées (SHAP, LIME)\",\n",
    "    \"Mettre en place une surveillance des performances en temps réel\",\n",
    "    \"Développer une interface utilisateur pour visualiser les prédictions\",\n",
    "    \"Implémenter un système de A/B testing pour les nouveaux modèles\",\n",
    "    \"Intégrer une base de données pour stocker les prédictions et feedback\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps):\n",
    "    print(f\"{i+1}. {step}\")\n",
    "\n",
    "print(\"\\nMerci d'avoir suivi ce notebook sur le pipeline complet de data engineering et machine learning !\")\n",
    "print(\"N'hésitez pas à adapter et étendre ce code pour vos propres projets.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
